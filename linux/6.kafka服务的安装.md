# kafka服务的安装
## 一、安装jdk1.8（若是新环境，未安装请先安装）

## 二、搭建zookeeper集群

### 1.进入安装目录
```
# cd /usr/local/
```

### 2.下载压缩包（版本自选）
```
# wget http://mirrors.hust.edu.cn/apache/zookeeper/zookeeper-3.4.13/zookeeper-3.4.13.tar.gz
```

### 3.解压安装包
```
# tar -zxvf zookeeper-3.4.13.tar.gz
```
### 4.进入conf目录
```
# cd zookeeper-3.4.13/conf/
# ls
configuration.xsl  log4j.properties  zoo_sample.cfg
```
### 5.拷贝zoo_samle.cfg为zoo.cfg
```
# cp zoo_sample.cfg zoo.cfg
# ls
configuration.xsl  log4j.properties  zoo.cfg  zoo_sample.cfg
```
### 6.打开zoo.cfg
```
# vi zoo.cfg
```
### 7.在zoo.cfg配置文件需要填入的信息
```
tickTime=2000
initLimit=10
syncLimit=5
dataDir=/opt/zookeeper/zkdata
clientPort=12181
//此处的IP就是你所操作的三台虚拟机的IP地址，每台虚拟机的zoo.cfg中都需要填入这三个地址。第一个端口是master和slave之间的通信端口，默认是2888，第二个端口是leader选举的端口，集群刚启动的时候选举或者leader挂掉之后进行新的选举的端口默认是3888
server.1=192.168.172.10:12888:13888
server.2=192.168.172.11:12888:13888
server.3=192.168.172.12:12888:13888
//server.1 这个1是服务器的标识也可以是其他的数字， 表示这个是第几号服务器，用来标识服务器，这个标识要写到dataDir目录下面myid文件里
```
### 8.在zkdata目录下放置myid文件：(上面zoo.cfg中的dataDir)
```
# mkdir zkdata
# vi myid
```
### 9.myid指明自己的id，对应上面zoo.cfg中server.后的数字，第一台的内容为1，第二台的内容为2，第二台的内容为3，内容如下
```
1
```
### 10.启动zookeeper
```
# cd /usr/local/zookeeper-3.4.13/bin/
# ./zkServer.sh start
```
### 11.查看zookeeper启动状态
```
# ./zkServer.sh status
JMX enabled by default
Using config: /opt/zookeeper/zookeeper-3.4.6/bin/../conf/zoo.cfg
Mode: follower
```

### 12.搭建集群遇到的问题
#### a.端口占用
```
若想启动的时候看日志，可以使用如下命令，需要注意的是，使用该命令即可启动了，不能再用 ./zkServer.sh start 命令，不然会报端口占用
# ./zkServer.sh start-foreground
```
#### b.须关闭防火墙，不然不能启动成功
```
# service iptables stop
# chkconfig iptables off
```
#### c.拒接连接
```
启动报错：拒绝连接，需开放所有zookeeper端口
# /sbin/iptables -I INPUT -p tcp --dport 12181 -j ACCEPT
# /sbin/iptables -I INPUT -p tcp --dport 12888 -j ACCEPT
# /sbin/iptables -I INPUT -p tcp --dport 13888 -j ACCEPT
```

## 二、搭建kafka集群

### 1.进入安装目录
```
# cd /usr/local/
```
### 2.下载压缩包（版本自选）
```
# wget http://mirrors.shu.edu.cn/apache/kafka/2.1.1/kafka_2.12-2.1.1.tgz
```
### 3.解压安装包
```
# tar -zxvf kafka_2.12-2.1.1.tgz
```
### 4.进入到config目录
```
# cd /usr/local/kafka_2.12-2.1.1/config/
```
### 5.修改配置文件server.properties
```
# vim server.properties
//这是这台虚拟机上的值，在另外两台虚拟机上应该是2或者3，这个值是唯一的，每台虚拟机或者叫服务器不能相同
broker.id=1
//设置本机IP和端口:这个IP地址也是与本机相关的，每台服务器上设置为自己的IP地址,端口号默认是9092，可以自己设置其他的
listeners=PLAINTEXT://192.168.172.10:9092
//在og.retention.hours=168下面新增下面三项
message.max.byte=5242880
default.replication.factor=2
replica.fetch.max.bytes=5242880
//指定日志位置
log.dirs=/data/kafka-logs
//设置日志删除
log.retention.hours=120
log.cleanup.polict=delete
log.segment.delete.delay.ms=1000
log.cleanup.interval.mins=1
log.retention.check.interval.ms=1000
//设置zookeeper的连接端口，zookeeper.connect可以设置多个值,多个值之间用逗号分隔
zookeeper.connect=192.168.172.12:12181,192.168.172.11:12181,192.168.172.10:12181
```
### 6.启动kafka集群
```
# cd /usr/local/kafka_2.12-2.1.1/bin/
# ./kafka-server-start.sh -daemon ../config/server.properties
```
### 7.创建topic来验证是否启动成功
```
# ./kafka-topics.sh --create --zookeeper 192.168.172.10:12181 --replication-factor 2 --partitions 1 --topic my-topic
# ./kafka-console-producer.sh --broker-list 192.168.172.10:9092 --topic my-topic
# ./kafka-console-consumer.sh --zookeeper 192.168.172.10:12181 --topic my-topic --from-beginning
```
如果生产，消费都正常，到此安装结束了。

### 8.搭建集群遇到的问题
#### a.重启kafka须删除2个日志文件，1个是kafka文件下的logs文件，第2个是配置文件中指定的存放kafka数据的日志文件

#### b.kafka运行过程中报错
```
报错：Attempting to send response via channel for which there is no open connection, connection id
解决：使用默认的socket_timeout_ms和offsets_channel_socket_timeout_ms
参考链接：https://www.jianshu.com/p/627b29f2236f
```
#### c.kafka运行过程中报错——"打开文件过多"(新集群搭建好之后就需要调整)
```
# vim /etc/profile
 增加 ulimit -n 10240
#source /etc/profile
```
#### c.kafka运行过程中报错——"存储空间不足"
```
解决方案：更换日志存储路径
注意：搭建集群时需注意服务器的磁盘空间，kafka存储日志的路径尽量挑选磁盘空间比较大的目录
```


### 9.kafka消费端应用系统遇到的问题
#### a.kafka集群部分节点宕机，生产者能继续生产，消费者不能正常消费
##### 1）首先需要将kafka的config文件夹下的server.properties中的复制因子参数调为3,然后重启服务
```
offsets.topic.replication.factor=3
transaction.state.log.replication.factor=3
transaction.state.log.min.isr=3

default.replication.factor=3
```
##### 2）然后执行下面的命令查看当前topic的备份数量，如果副本数不是3，则增加为3
```
./kafka-topics.sh --describe --zookeeper 10.10.200.50:12181,10.10.200.51:12181,10.10.200.52:12181|grep consumer_offsets
```
##### 3）如何增加副本数
```
1.进入kafka的bin目录，先创建规则json
cat > increase-replication-factor.json <<EOF
{"version":1, "partitions":[
{"topic":"__consumer_offsets","partition":0,"replicas":[1,2,3]},
{"topic":"__consumer_offsets","partition":1,"replicas":[1,2,3]},
{"topic":"__consumer_offsets","partition":2,"replicas":[1,2,3]},
{"topic":"__consumer_offsets","partition":3,"replicas":[1,2,3]},
{"topic":"__consumer_offsets","partition":4,"replicas":[1,2,3]},
{"topic":"__consumer_offsets","partition":5,"replicas":[1,2,3]},
{"topic":"__consumer_offsets","partition":6,"replicas":[1,2,3]},
{"topic":"__consumer_offsets","partition":7,"replicas":[1,2,3]},
{"topic":"__consumer_offsets","partition":8,"replicas":[1,2,3]},
{"topic":"__consumer_offsets","partition":9,"replicas":[1,2,3]},
{"topic":"__consumer_offsets","partition":10,"replicas":[1,2,3]},
{"topic":"__consumer_offsets","partition":11,"replicas":[1,2,3]},
{"topic":"__consumer_offsets","partition":12,"replicas":[1,2,3]},
{"topic":"__consumer_offsets","partition":13,"replicas":[1,2,3]},
{"topic":"__consumer_offsets","partition":14,"replicas":[1,2,3]},
{"topic":"__consumer_offsets","partition":15,"replicas":[1,2,3]},
{"topic":"__consumer_offsets","partition":16,"replicas":[1,2,3]},
{"topic":"__consumer_offsets","partition":17,"replicas":[1,2,3]},
{"topic":"__consumer_offsets","partition":18,"replicas":[1,2,3]},
{"topic":"__consumer_offsets","partition":19,"replicas":[1,2,3]},
{"topic":"__consumer_offsets","partition":20,"replicas":[1,2,3]},
{"topic":"__consumer_offsets","partition":21,"replicas":[1,2,3]},
{"topic":"__consumer_offsets","partition":22,"replicas":[1,2,3]},
{"topic":"__consumer_offsets","partition":23,"replicas":[1,2,3]},
{"topic":"__consumer_offsets","partition":24,"replicas":[1,2,3]},
{"topic":"__consumer_offsets","partition":25,"replicas":[1,2,3]},
{"topic":"__consumer_offsets","partition":26,"replicas":[1,2,3]},
{"topic":"__consumer_offsets","partition":27,"replicas":[1,2,3]},
{"topic":"__consumer_offsets","partition":28,"replicas":[1,2,3]},
{"topic":"__consumer_offsets","partition":29,"replicas":[1,2,3]},
{"topic":"__consumer_offsets","partition":30,"replicas":[1,2,3]},
{"topic":"__consumer_offsets","partition":31,"replicas":[1,2,3]},
{"topic":"__consumer_offsets","partition":32,"replicas":[1,2,3]},
{"topic":"__consumer_offsets","partition":33,"replicas":[1,2,3]},
{"topic":"__consumer_offsets","partition":34,"replicas":[1,2,3]},
{"topic":"__consumer_offsets","partition":35,"replicas":[1,2,3]},
{"topic":"__consumer_offsets","partition":36,"replicas":[1,2,3]},
{"topic":"__consumer_offsets","partition":37,"replicas":[1,2,3]},
{"topic":"__consumer_offsets","partition":38,"replicas":[1,2,3]},
{"topic":"__consumer_offsets","partition":39,"replicas":[1,2,3]},
{"topic":"__consumer_offsets","partition":40,"replicas":[1,2,3]},
{"topic":"__consumer_offsets","partition":41,"replicas":[1,2,3]},
{"topic":"__consumer_offsets","partition":42,"replicas":[1,2,3]},
{"topic":"__consumer_offsets","partition":43,"replicas":[1,2,3]},
{"topic":"__consumer_offsets","partition":44,"replicas":[1,2,3]},
{"topic":"__consumer_offsets","partition":45,"replicas":[1,2,3]},
{"topic":"__consumer_offsets","partition":46,"replicas":[1,2,3]},
{"topic":"__consumer_offsets","partition":47,"replicas":[1,2,3]},
{"topic":"__consumer_offsets","partition":48,"replicas":[1,2,3]},
{"topic":"__consumer_offsets","partition":49,"replicas":[1,2,3]}]
}
EOF

2.执行
./kafka-reassign-partitions.sh --zookeeper 10.10.200.50:12181,10.10.200.51:12181,10.10.200.52:12181 --reassignment-json-file increase-replication-factor.json --execute

3.验证是否执行成功
./kafka-reassign-partitions.sh --zookeeper 10.10.200.50:12181,10.10.200.51:12181,10.10.200.52:12181 --reassignment-json-file increase-replication-factor.json --verify
```
